{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_PyTorch_Dataset_Dataloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5yuXDh4W5zq"
      },
      "source": [
        "## **TODO:** Set the value of `URL` to the URL from your learning materials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk1soslgW8Xv"
      },
      "source": [
        "URL = None\n",
        "import os\n",
        "assert URL and (type(URL) is str), \"Be sure to initialize URL using the value from your learning materials\"\n",
        "os.environ['URL'] = URL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A_X7a8CW-UK"
      },
      "source": [
        "%%bash\n",
        "wget -q $URL -O ./data.zip\n",
        "mkdir -p data\n",
        "find *.zip | xargs unzip -o -d data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-mOZ6oR8q6K"
      },
      "source": [
        "## Use PyTorch `Dataset` and `Dataloader` with a structured dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oKWxWlmun66"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch as pt\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "pt.set_default_dtype(pt.float64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p8W9ZSL9I9w"
      },
      "source": [
        "Read the files that match `part-*.csv` from the `data` subdirectory into a Pandas data frame named `df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3fr0_i_YEFf"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "df = pd.concat(\n",
        "    pd.read_csv(file) for file in Path('data/').glob('part-*.csv')\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RULBK-A9X7D"
      },
      "source": [
        "## Explore the `df` data frame, including the column names, the first few rows of the dataset, and the data frame's memory usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vDc_ZNI9ilK"
      },
      "source": [
        "df[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY4lLvmL9kne"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-kVFhml9p0x"
      },
      "source": [
        "## Drop the `origindatetime_tr` column from the data frame. \n",
        "\n",
        "For now you are going to predict the taxi fare just based on the lat/lon coordinates of the pickup and the drop off locations. Remove the `origindatetime_tr` column from the data frame in your working dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhZpJTVZaas_"
      },
      "source": [
        "working_df = df.drop('origindatetime_tr', axis = 1)\n",
        "working_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aA0NkUA_x1M"
      },
      "source": [
        "## Sample 10% of your working dataset into a test dataset data frame\n",
        "\n",
        "* **hint:** use the Pandas `sample` function with the dataframe. Specify a value for the `random_state` to achieve reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsh_vPXiZr9J"
      },
      "source": [
        "test_df = working_df.sample(frac = 0.10, random_state = 42)\n",
        "test_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5FschugACN-"
      },
      "source": [
        "## Drop the rows that exist in your test dataset from the working dataset to produce a training dataset.\n",
        "\n",
        "* **hint** DataFrame's `drop` function can use index values from a data frame to drop specific rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT-b2IlIZ9FP"
      },
      "source": [
        "train_df = working_df.drop(index = test_df.index)\n",
        "train_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R0P1sBeAX15"
      },
      "source": [
        "## Define 2 Python lists: 1st for the feature column names; 2nd for the target column name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s62k_A-Ga-0x"
      },
      "source": [
        "FEATURES = ['origin_block_latitude','origin_block_longitude','destination_block_latitude','destination_block_longitude']\n",
        "TARGET = ['fareamount']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttQDA-m8AgQx"
      },
      "source": [
        "## Create `X` and `y` tensors with the values of your feature and target columns in the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX2dlZgpbA6I"
      },
      "source": [
        "X = pt.tensor(train_df[FEATURES].values)\n",
        "y = pt.tensor(train_df[TARGET].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQm_SJFDAqqn"
      },
      "source": [
        "## Create a `TensorDataset` instance with the `y` and `X` tensors (in that order)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffqTuheNbLpj"
      },
      "source": [
        "train_ds = TensorDataset(y, X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElrnaKEtAyEg"
      },
      "source": [
        "## Create a `DataLoader` instance specifying a custom batch size\n",
        "\n",
        "A batch size of `2 ** 18 = 262,144` should work well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1YyY4tgbalF"
      },
      "source": [
        "BATCH_SIZE = 2 ** 18\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
        "len(train_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA-3bXKABCW_"
      },
      "source": [
        "## Create a model using `nn.Linear`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vYGtKDeajQk"
      },
      "source": [
        "w = nn.Linear(len(FEATURES), 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9UvsR9gBGXj"
      },
      "source": [
        "## Create an instance of the `AdamW` optimizer for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPFF7EtFBKes"
      },
      "source": [
        "optimizer = pt.optim.AdamW(w.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz7LW-TnBNJu"
      },
      "source": [
        "## Declare your `forward`, `loss` and `metric` functions\n",
        "\n",
        "* **hint:** if you are tried of computing MSE by hand you can use `nn.functional.mse_loss` instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T3aWJEZdiVH"
      },
      "source": [
        "def forward(X):\n",
        "  return w(X)\n",
        "\n",
        "def loss(y_pred, y):\n",
        "  mse = nn.functional.mse_loss(y_pred, y)\n",
        "  return mse, mse.sqrt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfu2ejeoBfpQ"
      },
      "source": [
        "## Iterate over the batches returned by your `DataLoader` instance\n",
        "\n",
        "For every step of gradient descent, print out the MSE, RMSE, and the batch index\n",
        "* **hint:** you can use Python's `enumerable` for an iterable\n",
        "* **hint:** the batch returned by the `enumerable` has the same contents as your `TensorDataset` instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVjF8VYwbATZ"
      },
      "source": [
        "for batch_idx, batch in enumerate(train_dl):\n",
        "  y, X = batch\n",
        "  y_pred = forward(X)\n",
        "  mse, rmse = loss(y_pred, y)\n",
        "  mse.backward()\n",
        "  print(\"Loss: \", mse.item(), \" RMSE: \", rmse.item(), \" Batch Idx: \", batch_idx)\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVl44Jq5CApl"
      },
      "source": [
        "## Implement 10 epochs of gradient descent training\n",
        "\n",
        "For every step of gradient descent, printout the MSE, RMSE, epoch index, and batch index.\n",
        "\n",
        "* **hint:** you can call `enumerate(DataLoader)` repeatedly in a `for` loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHtI3TB8ewaF"
      },
      "source": [
        "for epoch in range(10):\n",
        "  for batch_idx, batch in enumerate(train_dl):\n",
        "    y, X = batch\n",
        "    y_pred = forward(X)\n",
        "    mse, rmse = loss(y_pred, y)\n",
        "    mse.backward()\n",
        "    print(\" Loss: \", mse.item(), \" RMSE: \", rmse.item(), \" Epoch: \", epoch, \" Batch Idx: \", batch_idx)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlrLCc3SCmuk"
      },
      "source": [
        "Copyright 2021 CounterFactual.AI LLC. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    }
  ]
}