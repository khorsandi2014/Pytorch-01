{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Demo_Autograd_GPU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBOV9uikT7uc"
      },
      "source": [
        "# Automatic differentiation with `autograd`\n",
        "Prior to `v0.4` PyTorch used the class `Variable` to record gradients. You had to wrap `Tensor`s in `Variable`s.\n",
        "`Variable`s behaved exactly like `Tensors`.\n",
        "\n",
        "With `v0.4` `Tensor` can record gradients directly if you tell it do do so, e.g. `torch.ones(3., requires_grad=True)`.\n",
        "There is no need for `Variable` anymore.\n",
        "Many tutorials still use `Variable`, be aware!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQw_9onZT7ud"
      },
      "source": [
        "Simply add `requires_grad=True` to the tensors you want to calculate the gradients for. PyTorch libraries such as `nn.Module` track gradients automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPqv0bLTT7ud"
      },
      "source": [
        "## Autograd is disabled by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul--BM_FT7ue"
      },
      "source": [
        "import torch as pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzXErpFaT7ui"
      },
      "source": [
        "x = pt.tensor(2.)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jTdbMw9T7ul"
      },
      "source": [
        "y = pt.tensor(3.)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Dj-jK9T7uo"
      },
      "source": [
        "z = x + y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3DtkKYdT7us"
      },
      "source": [
        "x.requires_grad, y.requires_grad, z.requires_grad,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PkbUXJ3T7uv"
      },
      "source": [
        "### `backward()` is equivalent to `backward(tensor(1.0))`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CphPQ6GGT7uv"
      },
      "source": [
        "z.requires_grad = True\n",
        "\n",
        "z.backward(pt.tensor(1.0)) \n",
        "\n",
        "z.grad, x.grad, y.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7-hmv06T7u1"
      },
      "source": [
        "## Enable autograd for a tensor with `requires_grad = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwc5X1PCT7u1"
      },
      "source": [
        "x = pt.tensor(2., requires_grad=True)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbkp7HNHT7u4"
      },
      "source": [
        "x.requires_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0enSu7tT7u7"
      },
      "source": [
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HPeGHInT7u9"
      },
      "source": [
        "## Non-leaf tensors acquire autograd by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P12uitAfT7u-"
      },
      "source": [
        "x = pt.tensor(2., requires_grad=True)\n",
        "z = pt.tensor(3.)\n",
        "\n",
        "#the product of x and z is a non-leaf tensor\n",
        "x * z\n",
        "\n",
        "x.requires_grad, z.requires_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU9IHc-_T7vB"
      },
      "source": [
        "y = x ** 2\n",
        "\n",
        "y.requires_grad, y.grad_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyXrocJjT7vD"
      },
      "source": [
        "y = x ** 2\n",
        "y.backward()\n",
        "\n",
        "x.grad, z.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVj1vknvT7vF"
      },
      "source": [
        "## Ignore autograd with `no_grad()`; useful for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zseHLNUT7vG"
      },
      "source": [
        "x = pt.tensor(2., requires_grad=True)\n",
        "\n",
        "with pt.no_grad():\n",
        "    y = x * x\n",
        "    print(y.grad_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f__vhfylT7vI"
      },
      "source": [
        "from torch import nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljRXhraLT7vL"
      },
      "source": [
        "model = nn.Linear(1, 1)\n",
        "model.weight.data\n",
        "model.bias.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh2sr7KyT7vO"
      },
      "source": [
        "<html><h2><font style=\"color:red\">GPU support is required</font> to demonstrate high performance tensor operations</h2></html>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cLNyRu7T7vO"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('In Google Colab select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "device = pt.device(\"cuda\" if pt.cuda.is_available() else \"cpu\")\n",
        "print('using device:', device)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_M0f-qtT7vR"
      },
      "source": [
        "When CUDA drivers are installed on the device, the following returns the CUDA capability profile documented by nVidia. For the capability profile v6.0 check out https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-6-x for information about the capability profile include the expected count of the Arithmetic Logic Units (ALUs).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IL5gDxjT7vS"
      },
      "source": [
        "pt.cuda.get_device_capability(device) if pt.cuda.is_available() else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_qYuE5yT7vU"
      },
      "source": [
        "## PyTorch tensor operations require tensors to reside on the same device \n",
        "* Use the `to` function to transfer a tensor to a specific device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ideDos-T7vU"
      },
      "source": [
        "a = pt.ones(100).to(\"cpu\")\n",
        "b = pt.ones(100).to(\"cuda\")\n",
        "a + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXed51SZT7vX"
      },
      "source": [
        "import timeit\n",
        "\n",
        "MAX_SIZE = 28\n",
        "sizes = [2 ** i for i in range(MAX_SIZE)]\n",
        "\n",
        "def benchmark_cpu_gpu(n):\n",
        "  for device in [\"cpu\", \"cuda\"]:\n",
        "    for size in sizes:\n",
        "      a = pt.randn(size).to(device)\n",
        "      b = pt.randn(size).to(device)\n",
        "      yield timeit.timeit(lambda: a + b, number = n)\n",
        "\n",
        "measurements = list(benchmark_cpu_gpu(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJbkEwlqT7vb"
      },
      "source": [
        "## Measure the performance ratio CPU / GPU, higher is faster GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-ln3pu6T7vb"
      },
      "source": [
        "cpu = measurements[:MAX_SIZE]\n",
        "gpu = measurements[MAX_SIZE:]\n",
        "ratios = [cpu[i] / gpu[i] for i in range(len(cpu))]\n",
        "ratios"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F4paf4ET7vf"
      },
      "source": [
        "## When adding tensors with `16,384` values and more, GPU can be `150x` faster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3WKlooPT7vf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rc('xtick', labelsize=20)\n",
        "plt.rc('ytick', labelsize=20)\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(sizes, ratios)\n",
        "plt.xscale(\"log\", basex=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGHWIcw9T7vh"
      },
      "source": [
        "## GPU performance on tensors with `8,192`  or fewer elements is on par with CPUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vnu-pY-T7vi"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.plot(sizes[:17], ratios[:17])\n",
        "plt.xscale(\"log\", basex=2);\n",
        "\n",
        "plt.rc('xtick', labelsize=20)\n",
        "plt.rc('ytick', labelsize=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBI5VGQNUI6m"
      },
      "source": [
        "Copyright 2021 CounterFactual.AI LLC. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    }
  ]
}