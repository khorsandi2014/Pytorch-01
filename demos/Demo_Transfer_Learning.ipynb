{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_Transfer_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mwpxa8lSj8M"
      },
      "source": [
        "# Download and unzip the cats vs. dogs data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oT4xJf55IVp"
      },
      "source": [
        "%%bash\n",
        "wget -q https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
        "unzip -q cats_and_dogs_filtered.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnWQQ41mSopl"
      },
      "source": [
        "# Start with the usual imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiY-HbSER-Xi"
      },
      "source": [
        "import time\n",
        "import os\n",
        "import copy\n",
        "import torch as pt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = pt.device(\"cuda:0\" if pt.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1uRSfbBSvS3"
      },
      "source": [
        "# The `torchvision.transforms` include image data augmentation features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbie1V6g6rkf"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(256),                                 \n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}\n",
        "data_transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYQLbBzBUNvs"
      },
      "source": [
        "The `datasets.ImageFolder` simplifies import of image data (as files) into a `DataLoader`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYsc5pjE6aWe"
      },
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "data_dir = 'cats_and_dogs_filtered'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'validation']}\n",
        "\n",
        "dataloaders = {x: pt.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=os.cpu_count())\n",
        "              for x in ['train', 'validation']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kbZDNYVU1Mj"
      },
      "source": [
        "# The methods in `torchvision.utils` implement common patterns in image data processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTcbxk-yTXSL"
      },
      "source": [
        "from torchvision import utils\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = utils.make_grid(inputs)\n",
        "\n",
        "def imshow(inp, title):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    # inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k9r2BexVJam"
      },
      "source": [
        "## Reusable, retrainable `MobileNet v2` architecture.\n",
        "\n",
        "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power.\n",
        "\n",
        "\n",
        "![](https://pytorch.org/assets/images/mobilenet_v2_1.png)\n",
        "\n",
        "![](https://pytorch.org/assets/images/mobilenet_v2_2.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2DDczVZ5d4P"
      },
      "source": [
        "from torchvision.models import mobilenet_v2\n",
        "model = mobilenet_v2(pretrained=True)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTin-RPQ9iNM"
      },
      "source": [
        "# Capture the number of the input features in the last `Linear` layer of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrfRjyKj5_j0"
      },
      "source": [
        "model.classifier[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhkuJigV8M5r"
      },
      "source": [
        "num_ftrs = model.classifier[1].in_features\n",
        "num_ftrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-xFd9M_Sq2E"
      },
      "source": [
        "# We need to set `requires_grad == False` to freeze the parameters so that the gradients are not computed in `backward()`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIqnuf467OyB"
      },
      "source": [
        "for p in model.parameters():\n",
        "  p.requires_grad = False  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkdu-kxuSyz8"
      },
      "source": [
        "# Parameters of newly created `Linear` layer have `requires_grad=true`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SOWd6wsSxKv"
      },
      "source": [
        "model.classifier[1] = pt.nn.Linear(num_ftrs, len(class_names))\n",
        "model.classifier[1].weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNYw7D-FV0Pi"
      },
      "source": [
        "# Implement a generic `train` function with a validation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHNAWzhHTgfV"
      },
      "source": [
        "def train(model, loss_fn, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with pt.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = pt.max(outputs, 1)\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += pt.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase}: Epoch {epoch}/{num_epochs - 1} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'validation' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXqvx83DSdzG"
      },
      "source": [
        "# Re-train the last layer while monitoring for accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zORbXhQNVtOF",
        "outputId": "85a8c9ec-ffd7-4d97-bad5-7de96966f502"
      },
      "source": [
        "model = model.to(device)\n",
        "optimizer = pt.optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9)\n",
        "scheduler = pt.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "model = train(model, \n",
        "              pt.nn.CrossEntropyLoss(),\n",
        "              #only parameters of final layer are being optimized \n",
        "              optimizer,\n",
        "              scheduler,\n",
        "              num_epochs=25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: Epoch 5/24 Loss: 0.5174 Acc: 0.8080\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}